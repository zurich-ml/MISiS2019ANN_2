{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tutorial: Computation Graphs and TensorFlow\n",
    "\n",
    "This is an introduction into the declarative programming paradigm used for computing by building a graph. The tutorial uses TensorFlow as an implementation.\n",
    "\n",
    "**DISCLAIMER**: this tutorial does **not** demonstrate how to write **efficient** or even **good** code in TensorFlow. It only serves the purpose of getting to understand certain (general and TensorFlow specific) **concepts**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Imperative paradigm or just pure python\n",
    "\n",
    "Before we go to the actual graphs, let's see what are non-graph computation.\n",
    "\n",
    "Calculations in Python are straight forward, for example as done below. To give it a name, this is an imperative paradigm, it's what most programs do. It means \"executing one line of code changes a value\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "const_5 = 5\n",
    "const_3 = 3\n",
    "sum_5_3 = const_5 + const_3\n",
    "\n",
    "const_7 = 7\n",
    "const_2 = 2\n",
    "sum_7_2 = const_7 + const_2\n",
    "\n",
    "prod_sums = sum_5_3 * sum_7_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We get an object out of the calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "prod_sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Unfortunately, this was not really efficient and we cannot deduce any more information form the output about where the number comes from \n",
    "\n",
    "*(equivalent to solving a simple physics exercise algebraic versus directly inserting the numbers into every variable)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Declarative paradigm and Lazy evaluation\n",
    "\n",
    "So it would be better, to \"build\" the calculation in the first place. Therefore, we need so-called \"lazy evaluation\", an object that first gets composed and run afterwards. This is a declarative programing style: we don't tell exactly what to do but rather what we wan't.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Optional (for coders): Lazy evaluation in python\n",
    "\n",
    "Let's do a very simple example of lazy evaluation (not really declarative actually) in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Two equivalent ways of writing a function in python\n",
    "def func():\n",
    "    return 42\n",
    "func = lambda: 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "const_5 = lambda: 5\n",
    "const_3 = lambda: 3\n",
    "sum_5_3 = lambda: const_5() + const_3()\n",
    "\n",
    "const_7 = lambda: 7\n",
    "const_2 = lambda: 2\n",
    "sum_7_2 = lambda: const_7() + const_2()\n",
    "\n",
    "prod_sums = lambda: sum_5_3() * sum_7_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "And now we did not yet evaluate anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "prod_sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To evaluate this object, we simply call it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "prod_sums()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The advantage over the previous approach: we _could_ use the information stored in prod_sums to improve the calculation _before_ we run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "lines = inspect.getsource(prod_sums)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building a Graph\n",
    "\n",
    "Let's now try to use the declarative paradigm and build a graph. The basic idea is that we don't tell what to _execute_ but just what we _want_. And in our case TensorFlow handles the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "const_5 = tf.constant(5.)\n",
    "const_3 = tf.constant(3.)\n",
    "sum_5_3 = tf.add(const_5, const_3)\n",
    "\n",
    "const_7 = tf.constant(7.)\n",
    "const_2 = tf.constant(2.)\n",
    "sum_7_2 = tf.add(const_7, const_2)\n",
    "\n",
    "prod_sums = tf.multiply(sum_5_3, sum_7_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "prod_sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Before we hat a function, that was our lazy evaluatable object, now it's a Tensor. Names don't matter here. This looks like the following: ![graph structure visualized](images/graph_structure_addition.png)\n",
    "\n",
    "We **just** created the graph now, the instructions on what we **want** but we did not tell that we want the result now. The object we have is an instruction, not a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "prod_sums_number = sess.run(prod_sums)  # this command actually executes the instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "prod_sums_number  # this is now a number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optional (for coders): did we really build a graph?\n",
    "Yes! Nothing else. Let's explore the graph by walking through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "prod_sums  # output from the operation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "prod_sums.op  # multiplies the inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "prod_sums.op.inputs[:]  # with for example input 0 from the op.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "prod_sums.op.inputs[0].op  # the first add with inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "prod_sums.op.inputs[0].op.inputs[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We have the whole definition at hand! It is a simple matter of implementations to, for example, check if a value depends on another (by recursively searching its inputs if it is there). Since TensorFlow also supports control flow operations, the whole process of building the graph is like building an AST (Abstract Syntax Tree) and therefore similar to writing compiler code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Automatic differentiation\n",
    "A very useful thing we can get out is the gradient. Using the chain rule (but that's an unimportant implementation detail), TensorFlow is able to compute the derivative of **any** node with respect to **any** other node. \n",
    "\n",
    "Let's look at the example we had before\n",
    "$$\n",
    "prod\\_sums(const\\_5) = (const\\_5 + const\\_3) \\cdot (const\\_7 + const\\_2)\n",
    "$$\n",
    "\n",
    "if we take the derivative with respect to $const\\_5$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial prod\\_sums}{\\partial const\\_5} = const\\_7 + const\\_2 = 9\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "derivate_sum_by_const5 = tf.gradients(prod_sums, const_5)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "derivate_sum_by_const5  # this is again a Tensor, an instruction how to do computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sess.run(derivate_sum_by_const5)  # as expected, this is 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Sidenote: this is taking the derivative of prod_sums with respect to the _node_ const_5 and _then_ evaluates this by inserting 5 into the node const_5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The above is not yet very useful, since we cannot change the values of a node. What we want is a more general structure that allows to define e.g. a model that can be changed with parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "const_2 = tf.constant(2.)\n",
    "var_1 = tf.Variable(name=\"variable1\", initial_value=3.)\n",
    "sess.run(var_1.initializer);  # just a thing needed to do, not important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# now create an operation as before\n",
    "power_var1_const2 = tf.pow(var_1, const_2)  # this is basically just a square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is analgous to defining\n",
    "\n",
    "$$\n",
    "\\text{power_var1_const2} = \\text{var_1}^{\\text{const_2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sess.run(power_var1_const2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# now let's change the value of the variable\n",
    "var_1.load(4.)  # you can also enter your own number here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# we can now either rerun the cell above or create a new cell here and enter the same command again.\n",
    "# For illustrative purpose the second is done here\n",
    "sess.run(power_var1_const2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's take the derivative with respect to the variable `var_1` (which is done equivalently as before).\n",
    "\n",
    "$$\n",
    "\\frac{\\partial power\\_var1\\_const2}{\\partial var_1} = const\\_2 \\cdot var\\_1 ^ {const\\_2 - 1}\n",
    "$$\n",
    "\n",
    "For $var\\_1 = 4$ (as we set above) we get 8.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "deriv_power_var = tf.gradients(power_var1_const2, var_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sess.run(deriv_power_var)  # as predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fitting a line\n",
    "Let's try to do a simple straight line fit. First we create some data in the next cell. If you're not familiar with Python and numpy, you can also skip this part. The important thing is: we create data along a straight line with a (random) slope. The actual slope is saved in `true_slope`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# generating some data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_events = 300\n",
    "true_slope = np.random.uniform(low=0.3, high=3)  # generate randomly a slope \n",
    "x_data = np.random.uniform(low=-10, high=10, size=n_events)  # generate points between -100 and 100\n",
    "y_clean = true_slope * x_data           # this is the function y = slope * x\n",
    "y_data = y_clean + np.random.normal(loc=0, scale=1.5, size=n_events)  # just adding some random noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# to visualize the data\n",
    "plt.plot(x_data, y_data, 'x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Making an ansatz\n",
    "\n",
    "Let's make an ansatz with a straight line.\n",
    "$$\n",
    "f(x) = y = slope \\cdot x\n",
    "$$\n",
    "\n",
    "Technically, we know several x and y from the data (fixed) and want to change slope so that the y is as close as possible to the y_data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# create the variable to be changed\n",
    "slope = tf.Variable(name=\"slope\", initial_value=1.)\n",
    "sess.run(slope.initializer)  # ignore line basically\n",
    "\n",
    "y = slope * x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Defining a loss\n",
    "\n",
    "We need to specify what it means that \"y_data should be as close as possible to y\". This is called the loss, which tells how \"bad\" the function is. As an example, we can take the squared distance of y and y_data.\n",
    "\n",
    "$$\n",
    "y_{dist} = y_{data} - f(x_{data}; slope)\n",
    "$$\n",
    "this is the differnce between the data and the prediction from our ansatz.\n",
    "\n",
    "$$\n",
    "squared\\_dist\\_loss = \\sum_i y_{dist\\ i}^2\n",
    "$$\n",
    "\n",
    "The goal is to minimize this expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# create a loss\n",
    "squared_dist_loss = tf.reduce_sum(tf.square(y - y_data))\n",
    "\n",
    "# since it can be useful, let's also create the instructions on how to compute the gradient\n",
    "grad_loss_wrt_slope = tf.gradients(squared_dist_loss, slope)[0]  # taking element 0 since it returns a list, not a scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Playground minimization\n",
    "\n",
    "Now we got everything: the instruction (operation) on how to compute the square distance loss (`squared_dist_loss`) and the parameter (`slope`) that parametrizes our ansatz. Use the two (three) cells below to minimize the loss and find the true slope.\n",
    "\n",
    "***Exercise***: call the loss cell, then load a different value, see how that changes the loss by running the first cell again and repeat until you find the minimum of it. Hint: if you finished and want to run the whole minimization for a new dataset and a new `true_slope`, restart the notebook and run everything again. Otherwise go for the optional advanced automatic minimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sess.run(squared_dist_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "slope.load(1.5)  # change between [0.3, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Using the gradient\n",
    "Being smart, we can use the gradient information as well. Since it point into the direction of the steepest **ascent**, using the negative of it points into the direction of the **descent**. This tells us in which direction to change the parameter.\n",
    "\n",
    "$$\n",
    "grad\\_loss\\_wrt\\_slope = \\frac{\\partial squared\\_dist\\_loss}{\\partial slope}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sess.run(grad_loss_wrt_slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# uncomment the below to see the true value\n",
    "# true_slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Optional (advanced): automatize the minimization\n",
    "\n",
    "Why not create a while loop and automatize this? If you feel comfortable enough with the above and python, implement a loop. Hint: a criteria to stop the loop (convergence criteria) could be the absolute value of the gradient being smaller than a certain stopping value. And don't make too big steps, rather too small ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# do your minimization here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Changing our goal\n",
    "\n",
    "We created a simple loss using the sum of the squared distances. However, we can create way more sophisticated losses by adding more terms to it. For example if we do know that the slope is close to 1 (e.g. from other measurements), we can add an additional term that penalizes if `slope` is not close to 1.\n",
    "\n",
    "*(30'000 is just to increase the effect and make it visible)*\n",
    "$$\n",
    "slope_{constr} = (slope - slope_{assumed}) ^ 2 \\cdot 20'000\n",
    "$$\n",
    "\n",
    "*(In physics and fitting, this is usually called \"adding a constraint to a parameter\")*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "slope_assumed = tf.constant(1.)  # any number within the boundaries [0.3, 3]\n",
    "slope_constr = tf.square(tf.subtract(slope, slope_assumed)) * 20000\n",
    "squared_dist_constr = squared_dist_loss + slope_constr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We minimized by hand before (and it was cumbersome) but there are already pre-built minimizers (and in practice, we gonna **always** use them). So let's try one. There are a few different ones and some converge better for certain problems then others. In general, `Adam` performs overall very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# here we create an instance of the optimizer, this needs to be done once\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "minimize_op = optimizer.minimize(squared_dist_constr)  # as before, this is also just an operation!\n",
    "sess.run(tf.variables_initializer(optimizer.variables()))  # just a necessity, not important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we can run the optimization. `minimize_op` just performs one step of the optimization (one step along the negative gradient) so we do it a couple of times (e.g. 1'000). Of course, a while loop with a good convergence criteria would be more suited in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# execute the minimization operation here\n",
    "for _ in range(1000):\n",
    "    sess.run(minimize_op)  # this is only one minimization step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# to check the value of the slope, we also need to run it\n",
    "sess.run(slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# uncomment to see the true slope. This will NOT coincide with our slope and should be closer to 1\n",
    "# since we added the constraint\n",
    "true_slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optional: optimizer on the previous loss\n",
    "\n",
    "We can also minimize the original squared distance loss with an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "minimize_op_squared_dist = optimizer.minimize(squared_dist_loss)  # as before, this is also just an operation!\n",
    "sess.run(tf.variables_initializer(optimizer.variables()))  # just a necessity, not important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# execute the minimization operation here\n",
    "for _ in range(1000):\n",
    "    sess.run(minimize_op_squared_dist)  # this is only one minimization step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "sess.run(slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "true_slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
