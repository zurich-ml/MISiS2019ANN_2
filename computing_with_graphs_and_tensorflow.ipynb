{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation Graphs and TensorFlow\n",
    "\n",
    "This is an introduction into the declarative programming paradigm used for computing by building a graph. The tutorial uses TensorFlow as an implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imperative paradigm or just pure python\n",
    "\n",
    "Before we go to the actual graphs, let's see what are non-graph computation.\n",
    "\n",
    "Calculations in Python are straight forward, for example as done below. To give it a name, this is an imperative paradigm, it's what most programs do. It means \"executing one line of code changes a value\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_5 = 5\n",
    "const_3 = 3\n",
    "sum_5_3 = const_5 + const_3\n",
    "\n",
    "const_7 = 7\n",
    "const_2 = 2\n",
    "sum_7_2 = const_7 + const_2\n",
    "\n",
    "prod_sums = sum_5_3 * sum_7_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get an object out of the calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod_sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, this was not really efficient and we cannot deduce any more information form the output about where the number comes from \n",
    "\n",
    "*(equivalent to solving a simple physics exercise algebraic versus directly inserting the numbers into every variable)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declarative paradigm and Lazy evaluation\n",
    "\n",
    "So it would be better, to \"build\" the calculation in the first place. Therefore, we need so-called \"lazy evaluation\", an object that first gets composed and run afterwards. This is a declarative programing style: we don't tell exactly what to do but rather what we wan't.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional (for coders): Lazy evaluation in python\n",
    "\n",
    "Let's do a very simple example of lazy evaluation (not really declarative actually) in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two equivalent ways of writing a function in python\n",
    "def func():\n",
    "    return 42\n",
    "func = lambda: 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_5 = lambda: 5\n",
    "const_3 = lambda: 3\n",
    "sum_5_3 = lambda: const_5() + const_3()\n",
    "\n",
    "const_7 = lambda: 7\n",
    "const_2 = lambda: 2\n",
    "sum_7_2 = lambda: const_7() + const_2()\n",
    "\n",
    "prod_sums = lambda: sum_5_3() * sum_7_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we did not yet evaluate anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate this object, we simply call it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_sums()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage over the previous approach: we _could_ use the information stored in prod_sums to improve the calculation _before_ we run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "lines = inspect.getsource(prod_sums)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Graph\n",
    "\n",
    "Let's now try to use the declarative paradigm and build a graph. The basic idea is that we don't tell what to _execute_ but just what we _want_. And in our case TensorFlow handles the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_5 = tf.constant(5.)\n",
    "const_3 = tf.constant(3.)\n",
    "sum_5_3 = tf.add(const_5, const_3)\n",
    "\n",
    "# or even simpler\n",
    "const_7 = tf.constant(7.)\n",
    "const_2 = tf.constant(2.)\n",
    "sum_7_2 = tf.add(const_7, const_2)\n",
    "\n",
    "prod_sums = tf.multiply(sum_5_3, sum_7_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Mul:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod_sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we hat a function, that was our lazy evaluatable object, now it's a Tensor. Names don't matter here. This looks like the following: ![graph structure visualized](images/graph_structure_addition.png)\n",
    "\n",
    "We **just** created the graph now, the instructions on what we **want** but we did not tell that we want the result now. The object we have is an instruction, not a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_sums_number = sess.run(prod_sums)  # this command actually executes the instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod_sums_number  # this is now a number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional (for coders): did we really build a graph?\n",
    "Yes! Nothing else. Let's explore the graph by walking through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_sums  # output from the operation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_sums.op  # multiplies the inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_sums.op.inputs[:]  # with for example input 0 from the op.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_sums.op.inputs[0].op  # the first add with inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_sums.op.inputs[0].op.inputs[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the whole definition at hand! It is a simple matter of implementations to, for example, check if a value depends on another (by recursively searching its inputs if it is there). Since TensorFlow also supports control flow operations, the whole process of building the graph is like building an AST (Abstract Syntax Tree) and therefore similar to writing compiler code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation\n",
    "A very useful thing we can get out is the gradient. Using the chain rule (but that's an unimportant implementation detail), TensorFlow is able to compute the derivative of **any** node with respect to **any** other node. \n",
    "\n",
    "Let's look at the example we had before\n",
    "$$\n",
    "prod\\_sums(const\\_5) = (const\\_5 + const\\_3) \\cdot (const\\_7 + const\\_2)\n",
    "$$\n",
    "\n",
    "if we take the derivative with respect to $const\\_5$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial prod\\_sums}{\\partial const\\_5} = const\\_7 + const\\_2\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivate_sum_by_const5 = tf.gradients(prod_sums, const_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivate_sum_by_const5  # this is again a Tensor, an instruction how to do computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(derivate_sum_by_const5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sidenote: this is taking the derivative of prod_sums with respect to the _node_ const_5 and _then_ evaluates this by inserting 5 into the node const_5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is not yet very useful, since we cannot change the values of a node. What we want is a more general structure that allows to define e.g. a model that can be changed with parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_2 = tf.constant(2.)\n",
    "var_1 = tf.Variable(name=\"variable1\", initial_value=3., use_resource=True)\n",
    "sess.run(var_1.initializer)  # just a thing needed to do, not important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create an operation as before\n",
    "power_const_var = tf.pow(var_1, const_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(power_const_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's change the value of the variable\n",
    "var_1.load(4.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now either rerun the cell above or create a new cell here and enter the same command again.\n",
    "# For illustrative purpose the second is done here\n",
    "sess.run(power_const_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and we can also take the derivative of course with respect to the variable\n",
    "deriv_power_var = tf.gradients(power_const_var, var_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(deriv_power_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a line\n",
    "Let's try to do a simple straight line fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating some data\n",
    "n_events = 300\n",
    "true_slope = np.random.uniform(low=0.3, high=3)  # generate randomly a slope \n",
    "x_data = np.random.uniform(low=-10, high=10, size=n_events)  # generate points between -100 and 100\n",
    "y_clean = true_slope * x_data           # this is the function y = slope * x\n",
    "y_data = y_clean + np.random.normal(loc=0, scale=1, size=n_events)  # just adding some random noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to visualize the data\n",
    "plt.plot(x_data, y_data, 'x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO explanation with Ansatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the variable to be changed\n",
    "slope = tf.Variable(name=\"slope\", initial_value=1.)\n",
    "sess.run(tf.global_variables_initializer())  # ignore line basically\n",
    "\n",
    "# x_data_tf = tf.convert_to_tensor(x_data)\n",
    "y = slope * x_data\n",
    "\n",
    "# create a loss\n",
    "squared_dist_loss = tf.reduce_sum(tf.square(y - y_data))\n",
    "\n",
    "# since it can be useful, let's also create the instructions on how to compute the gradient\n",
    "grad_slope = tf.gradients(squared_dist_loss, slope)[0]  # taking element 0 since it returns a list, not a scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground minimization\n",
    "\n",
    "Now we got everything: the instruction on how to compute the chi square loss and the parameter (`slope`) that changes our model. Use the two (three) cells below to minimize the loss and find the true slope.\n",
    "\n",
    "***Exercise***: call the loss cell, then load a different value, see how it changes and repeat until you find the minimum of it. Hint: if you wanna run again, restart the notebook and run everything again. This will give you a new value for slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(squared_dist_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope.load(1.233)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "being smart, we can use the gradient information as well. Since it point into the direction of the steepest **ascent**, using the negative of it points into the direction of the **descent**. This tells us in which direction to change the parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(grad_slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the below to see the true value\n",
    "true_slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional (advanced): automatize the minimization\n",
    "\n",
    "Why not create a while loop and automatize this? If you feel comfortable enough with the above and python, implement a loop. Hint: a criteria to stop the loop (convergence criteria) could be the absolute value of the gradient being smaller than a certain stopping value. And don't make too big steps, rather too small ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do your minimization here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing our goal\n",
    "\n",
    "We created a simple loss using the sum of the squared distances. However, we can create way more sophisticated losses by adding more terms to it. For example if we'd know that the slope is close to 1 (or take another number if your true slope is already close to 1), we can add an additional term that penalizes if `slope` is far away.\n",
    "\n",
    "*(while Deep Learning uses the general expression of building a loss from components, in physics and fitting, this is usually called \"adding a constraint to a parameter\". There are many names for \"the same\")*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_we_assume = tf.constant(1.)  # any number within the boundaries [0.3, 3]\n",
    "slope_constr = tf.square(500 * tf.subtract(slope, slope_we_assume))\n",
    "squared_dist_constr = squared_dist_loss + slope_constr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We minimized by hand before (and it was cumbersome) but there are already pre-built minimizers (and in practice, we gonna **always** use them). So let's try one. There are a few different ones and some converge better for certain problems then others. In general, `Adam` performs overall very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=3.)  # here we create an instance of the optimizer, this needs to be done once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimize_op = optimizer.minimize(squared_dist_constr)  # as before, this is also just an operation!\n",
    "sess.run(tf.variables_initializer(optimizer.variables()))  # just a necessity, not important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute the minimization operation here\n",
    "for _ in range(1000):\n",
    "    sess.run(minimize_op)  # this is only one minimization step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check the value of the slope, we also need to run it\n",
    "sess.run(slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to see the true slope\n",
    "true_slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
