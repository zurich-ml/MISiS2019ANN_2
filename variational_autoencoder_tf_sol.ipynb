{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Coding a Variational AutoEncoder in Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The Variational AutoEncoder network is a network architecture that aims to compress a high dimensional input vector to a vector in a lower dimensional space (encoding). In order to do so the network compresses and decompresses the input during the learning process.\n",
    "\n",
    "![Center](images/vae_arch.png)\n",
    "\n",
    " We call the input of the network $X$ and the output $X'$. The train the network to obtain the optimal compression, i.e. when the difference between the input $X$ and the decompressed output $X'$ is minimum. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Let's first import some useful libraries, and define the Leaky_ReLU function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "def lrelu(X, alpha=0.1):\n",
    "    return tf.maximum(alpha*X, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "Let's also import the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, _), (X_test, _) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.astype(np.float32).reshape(-1,28*28)/X_train.max()\n",
    "X_test = X_test.astype(np.float32).reshape(-1,28*28)/X_train.max()\n",
    "\n",
    "X_test_plot=X_test\n",
    "X_train = (X_train>0.5).astype(np.float32)\n",
    "X_test = (X_test>0.5).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and draw a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Sample: \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAABOhJREFUeJzt3bGOTVscwOG7r6uhkxAdlYyEIFFISDQq0WhVHkDiPbQeQSWZRCFCS6mhFJVCRaEiGrYXmLMmd86ZM+P8vq88/6zZq/llJbNyzp7mef4H2Hz/HvQGgPUQO0SIHSLEDhFih4j/1vmwaZr86x/22TzP006fO9khQuwQIXaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4Ra31lMz3nzp1bOPvw4cNw7cOHD4fzx48f72lPVU52iBA7RIgdIsQOEWKHCLFDhNghwj07++rKlSsLZ79//x6u/fz586q3k+ZkhwixQ4TYIULsECF2iBA7RIgdItyzs68uX768cPb9+/fh2mfPnq16O2lOdogQO0SIHSLEDhFihwixQ4SrN5Zy4cKF4fzBgwcLZ0+ePFn1dhhwskOE2CFC7BAhdogQO0SIHSLEDhHu2VnK1tbWcH78+PGFs6dPn656Oww42SFC7BAhdogQO0SIHSLEDhFih4hpnuf1PWya1vcw1uLt27fD+cmTJxfOdvsu/G4/Nc3O5nmedvrcyQ4RYocIsUOE2CFC7BAhdogQO0T4PjtDZ8+eHc6vXr06nH/8+HHhzD36ejnZIULsECF2iBA7RIgdIsQOEWKHCPfsDN28eXOp9V+/fl3RTliWkx0ixA4RYocIsUOE2CFC7BDh6o2hixcvLrX+0aNHK9oJy3KyQ4TYIULsECF2iBA7RIgdIsQOEV7ZHHft2rXh/MWLF8P5p0+fhvPr168vnP38+XO4lr3xymaIEztEiB0ixA4RYocIsUOE2CHC99njbt26NZyfOHFiOH/16tVw7i798HCyQ4TYIULsECF2iBA7RIgdIsQOEe7Z4y5dujSc7/Z7B9vb26vcDvvIyQ4RYocIsUOE2CFC7BAhdogQO0T43fgNd/r06eH8/fv3w/m3b9+G8/Pnz//vPbG//G48xIkdIsQOEWKHCLFDhNghwldcN9z9+/eH81OnTg3nL1++XOFuOEhOdogQO0SIHSLEDhFihwixQ4TYIcI9+4Y7c+bMUut3+4orfw8nO0SIHSLEDhFihwixQ4TYIULsEOGefcPduXNnqfXPnz9f0U44aE52iBA7RIgdIsQOEWKHCLFDhNghwj37Brhx48bC2W6vbKbDyQ4RYocIsUOE2CFC7BAhdohw9bYB7t69u3B25MiR4dp3794N52/evNnTnjh8nOwQIXaIEDtEiB0ixA4RYocIsUOEe/a/wLFjx4bz27dv7/lvb29vD+e/fv3a89/mcHGyQ4TYIULsECF2iBA7RIgdIsQOEdM8z+t72DSt72Eb5OjRo8P569evF86+fPkyXHvv3r3h/MePH8M5h888z9NOnzvZIULsECF2iBA7RIgdIsQOEWKHCPfssGHcs0Oc2CFC7BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQO0SIHSLW+spm4OA42SFC7BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4RYoeIP8ieku2KR0d8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = 2 #chose one sample in the training set\n",
    "\n",
    "\n",
    "\n",
    "print('\\n Sample: \\n')\n",
    "plt.imshow(X_test_plot[sample].reshape(28,28), cmap='gray');\n",
    "plt.axis('off');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Layer class definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class provides the definition of a dense layer. A dense layer is characterized by its trainable weights:\n",
    "\n",
    "- a matrix W of dimensions (m_input, m_output)\n",
    "- a bias vector b of dimensions (m_output)\n",
    "\n",
    "The Layer class possesses the ```forward``` method which performs the linear forward propagation:\n",
    "\n",
    "$$\n",
    "Z_{out}= X_{in} \\times W(m_{in},m_{out})+b(m_{out})\n",
    "$$\n",
    "\n",
    "Code along with us and follow the provided instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \n",
    "    def __init__(self, mi, mo, name):\n",
    "        \n",
    "        \n",
    "        self.W = tf.get_variable(name='W_'+name,        \n",
    "                                 shape=(mi, mo),          \n",
    "                                 initializer=tf.glorot_uniform_initializer()) \n",
    "                                                                                                               \n",
    "        \n",
    "        self.b = tf.get_variable(name='b_'+name, \n",
    "                                 shape=(mo,),              \n",
    "                                 initializer=tf.zeros_initializer(),) \n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        \n",
    "        Z=tf.matmul(X,self.W)+self.b\n",
    "        \n",
    "        return Z\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Creating the AutoEncoder Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build the AutoEncoder as a class possessing several methods:\n",
    "\n",
    "```\n",
    "class AutoEncoder():\n",
    "\n",
    "    def __init__():\n",
    "    \n",
    "    def build_network():\n",
    "    \n",
    "    def decode():\n",
    "    \n",
    "    def encode():\n",
    "    \n",
    "    def fit():\n",
    "    \n",
    "```\n",
    "\n",
    "Code along with us and follow the provided instructions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(object):\n",
    "    \n",
    "    def __init__(self, input_dim, layer_dims, \n",
    "                learning_rate, epochs, batch_size):\n",
    "        \n",
    "    \n",
    "        self.layer_dims = layer_dims \n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.latent_dims = layer_dims[-1]\n",
    "\n",
    "        #Decoder dimensions\n",
    "        self.reversed_layer_dims= list(reversed(layer_dims)) \n",
    "        \n",
    "        self.X_in = tf.placeholder(tf.float32, shape=(None, input_dim))\n",
    "        \n",
    "        \n",
    "        self.encoder_layers = []\n",
    "        self.decoder_layers = []\n",
    "        \n",
    "        self.build_network(input_dim)\n",
    "        \n",
    "    \n",
    "    def build_network(self, input_dim):\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        ENCODER LAYERS\n",
    "        \"\"\"\n",
    "        \n",
    "        mi = input_dim    #input size\n",
    "        count=0           \n",
    "        \n",
    "        for mo in self.layer_dims[:-1]:      \n",
    "                                             \n",
    "            \n",
    "            layer = Layer(mi, mo, str(count))    \n",
    "            self.encoder_layers.append(layer)\n",
    "            count+=1                         \n",
    "            mi = mo                          \n",
    "                                             \n",
    "\n",
    "        self.encoder_layer_mu = Layer(mi, self.latent_dims, 'encoder_mu')\n",
    "        self.encoder_layer_si = Layer(mi, self.latent_dims, 'encoder_si')\n",
    "        \n",
    "            \n",
    "        \"\"\"\n",
    "        DECODER LAYERS\n",
    "        \"\"\"\n",
    "\n",
    "        mi = self.latent_dims\n",
    "        for mo in self.reversed_layer_dims:   \n",
    "            \n",
    "            layer = Layer(mi, mo, str(count))      \n",
    "            self.decoder_layers.append(layer) \n",
    "            count+=1                          \n",
    "            mi = mo                           \n",
    "            \n",
    "        \n",
    "        output_layer = Layer(mi,input_dim, str(count))    \n",
    "        self.decoder_layers.append(output_layer)    \n",
    "        \n",
    "    \n",
    "        \"\"\"\n",
    "        \n",
    "        BUILDING THE GRAPH\n",
    "        \n",
    "        \"\"\"\n",
    "        #Encode the input\n",
    "        self.Z_dist = self.encode(self.X_in)\n",
    "        \n",
    "        #get z_dist = q(z | X).sample()\n",
    "        Z_dist_sample= self.Z_dist.sample()\n",
    "        \n",
    "        #Decode \n",
    "        logits = self.decode(Z_dist_sample)\n",
    "        \n",
    "        #self.X_hat_dist = tfd.Bernoulli(logits)\n",
    "        self.X_hat_dist = tfd.Independent(tfd.Bernoulli(logits), 2)\n",
    "        \n",
    "        #get X_hat = p(X_hat | z_dist).sample()\n",
    "        #self.posterior_predictive = self.X_hat_dist.sample()\n",
    "        self.posterior_predictive_probs = tf.nn.sigmoid(logits)\n",
    "        \n",
    "        # Generate one sample\n",
    "        Z_norm = tfd.MultivariateNormalDiag(loc = tf.zeros(self.latent_dims),\n",
    "                                           scale_diag = tf.ones(self.latent_dims))\n",
    "        \n",
    "        \n",
    "        logits = self.decode(Z_norm.sample(1))\n",
    "        \n",
    "        self.prior_predictive_probs = tf.nn.sigmoid(logits)\n",
    "        \n",
    "        \n",
    "        self.Z_input = tf.placeholder(tf.float32, shape=(None, self.latent_dims))\n",
    "        logits = self.decode(self.Z_input)\n",
    "        self.prior_predictive_from_input_probs = tf.nn.sigmoid(logits) \n",
    "        \n",
    "        \"\"\"\n",
    "        LOSS\n",
    "        \"\"\"\n",
    "        \n",
    "        # KL divergence\n",
    "        kl_div = tfd.kl_divergence(self.Z_dist, Z_norm)\n",
    "        print(logits.get_shape())\n",
    "        print(self.X_in.get_shape())\n",
    "        exp_log_likelihood = self.X_hat_dist.log_prob(self.X_in)\n",
    "        \n",
    "        #Expected log likelihood\n",
    "        self.loss = - tf.reduce_mean(exp_log_likelihood-kl_div)\n",
    "        # reduce_sum converts a tensor to a scalar sum of its components\n",
    "                    \n",
    "        \"\"\"\n",
    "        \n",
    "        OPTIMIZATION ALGORITHM\n",
    "        \n",
    "        \"\"\"\n",
    "        # write code here\n",
    "        self.train_op = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "                                                #set the learning rate                #what to minimize\n",
    "        \n",
    "    \"\"\"\n",
    "    We now define the last two functions: encode and decode which we used previously\n",
    "    in STEP 2 \n",
    "    \"\"\"\n",
    "    # write code here\n",
    "    def encode(self, X):                                \n",
    "                                                        \n",
    "        output=X\n",
    "        \n",
    "        for layer in self.encoder_layers:          \n",
    "                                                        \n",
    "            output = layer.forward(output)              \n",
    "            output = lrelu(output)\n",
    "\n",
    "                                       \n",
    "        mu = self.encoder_layer_mu.forward(output)                     \n",
    "        sigma = tf.nn.softplus(self.encoder_layer_si.forward(output))+1e-6\n",
    "        \n",
    "        return tfd.MultivariateNormalDiag(loc=mu, scale_diag=sigma)                    \n",
    "        \n",
    "    def decode(self,Z):                                 \n",
    "                                                        \n",
    "        output=Z\n",
    "        \n",
    "        for layer in self.decoder_layers[:-1]:               \n",
    "\n",
    "            output = layer.forward(output)\n",
    "            output = tf.nn.sigmoid(output)\n",
    "            \n",
    "        logits = self.decoder_layers[-1].forward(output)\n",
    "        return logits\n",
    "    \n",
    "    \n",
    "    def fit(self, X, show_fig=True):                    \n",
    "         \n",
    "        losses = []                                     \n",
    "        n_batches = len(X) // self.batch_size\n",
    "        \n",
    "        print(\"Training AutoEncoder\")\n",
    "        print(\"n_batches:\", n_batches)\n",
    "        for i in range(self.epochs):\n",
    "            \n",
    "            print(\"epoch:\", i)\n",
    "            np.random.shuffle(X)\n",
    "        \n",
    "            for j in range(n_batches):                 \n",
    "            \n",
    "                X_batch = X[j*self.batch_size:(j+1)*self.batch_size] \n",
    "                _, l, = self.session.run((self.train_op, self.loss), \n",
    "                                         feed_dict={self.X_in: X_batch}) #running one step of gradient\n",
    "                l /= self.batch_size                                     #minimization\n",
    "\n",
    "                losses.append(l)\n",
    "                \n",
    "                if j % 500*self.batch_size == 0:        \n",
    "\n",
    "                    print(\"iter: %d, cost: %.3f\" % (j, l))\n",
    "            \n",
    "        if show_fig:\n",
    "            plt.plot(losses)\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    def predict(self, X):                              # function to test the network once training\n",
    "                                                       # is completed\n",
    "        return self.session.run(self.posterior_predictive_probs, feed_dict={self.X_in: X})\n",
    "    \n",
    "    def generate_sample(self):\n",
    "        \n",
    "        one_sample=self.session.run(self.prior_predictive_probs)\n",
    "\n",
    "        return one_sample\n",
    "    \n",
    "    def generate_sample_from_input(self, z):\n",
    "        \n",
    "        one_sample=self.session.run(self.prior_predictive_from_input_probs, feed_dict={self.Z_input:z})\n",
    "\n",
    "        return one_sample\n",
    "   \n",
    "    def set_session(self, session):\n",
    "        \n",
    "        self.session = session\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Test the AutoEncoder\n",
    "\n",
    "Now that the AutoEncoder class is defined, we create our own AutoEncoder and test how it works. As usual follow our coding and use the provided instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 784)\n",
      "(?, 784)\n",
      "Training AutoEncoder\n",
      "n_batches: 600\n",
      "epoch: 0\n",
      "iter: 0, cost: inf\n",
      "iter: 500, cost: inf\n",
      "epoch: 1\n",
      "iter: 0, cost: inf\n",
      "iter: 500, cost: inf\n",
      "epoch: 2\n",
      "iter: 0, cost: inf\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7b6b3dfed591>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mdae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Set it for the AutoEncoder you defined previously\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mdae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# Call the fit method that you defined previously\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-7cf5aced7bf4>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, show_fig)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 _, l, = self.session.run((self.train_op, self.loss), \n\u001b[0;32m--> 172\u001b[0;31m                                          feed_dict={self.X_in: X_batch}) #running one step of gradient\n\u001b[0m\u001b[1;32m    173\u001b[0m                 \u001b[0ml\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m                                     \u001b[0;31m#minimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "_, input_dimensions = X_train.shape     # Lets get the input_dimensions from the dataset\n",
    "tf.reset_default_graph()                # Resetting any previously defined graph\n",
    "\n",
    "\"\"\"\n",
    "Create the AutoEncoder object\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#write code here                \n",
    "dae = AutoEncoder(input_dim=input_dimensions, # Create your very own network here!!\n",
    "                  \n",
    "                  layer_dims=[500,300,100,2],   # Play with the layers' list, it will \n",
    "                                              # automatically define the intermediate\n",
    "                                              # layers.\n",
    "                  learning_rate=0.001,       # Set some values for the learning rate\n",
    "                  epochs=20, batch_size=100)  # the epochs and the batch_size\n",
    "\n",
    "\n",
    "\n",
    "init_op=tf.global_variables_initializer() # Define the network's variables initializer\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.10)\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options )) as sess: #Create a tensorflow session\n",
    "    \n",
    "    sess.run(init_op)     # Initialize the network variables in this sess\n",
    "    dae.set_session(sess) # Set it for the AutoEncoder you defined previously\n",
    "    \n",
    "    dae.fit(X_train)      # Call the fit method that you defined previously\n",
    "    \n",
    "\n",
    "    # Testing the created network performances on the test set!\n",
    "    done = False\n",
    "    while not done:\n",
    "        i = np.random.choice(len(X_test))         # pick a random sample of the test set\n",
    "        x = X_test[i]                             \n",
    "        y = dae.predict([x]).reshape(28, 28)      # propagate it through the network\n",
    "\n",
    "        plt.subplot(1,2,1)                        # Plot the original and the reconstructed\n",
    "        plt.imshow(X_test_plot[i].reshape(28, 28), cmap='gray')\n",
    "        plt.title(\"Original\")\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.imshow(y, cmap='gray')\n",
    "        plt.title(\"Reconstruction\")\n",
    "        plt.show()\n",
    "\n",
    "        ans = input(\"Generate another?\")          # Stop the while loop with 'n' or 'N'\n",
    "        if ans and ans[0] in ('n' , 'N'):\n",
    "              done = True\n",
    "                \n",
    "    done = False\n",
    "    while not done:\n",
    "                       \n",
    "        y = dae.generate_sample();     # propagate it through the network\n",
    "\n",
    "        plt.imshow(y.reshape(28, 28), cmap='gray');\n",
    "        plt.title(\"Reconstruction\")\n",
    "        plt.show()\n",
    "\n",
    "        ans = input(\"Generate another?\")          # Stop the while loop with 'n' or 'N'\n",
    "        if ans and ans[0] in ('n' , 'N'):\n",
    "              done = True\n",
    "                \n",
    "#   n = 10 #number of images per side\n",
    "#   x_values = np.linspace(-3,3,n)\n",
    "#   y_values = np.linspace(-3,3,n)\n",
    "#   \n",
    "#   image = np.empty((28*n,28*n))\n",
    "#   \n",
    "#   Z=[]\n",
    "#   for i, x in enumerate(x_values):\n",
    "#       for j, y in enumerate(y_values):\n",
    "#           z=[x, y]\n",
    "#           Z.append(z)\n",
    "#       \n",
    "#   X_recon = dae.generate_sample_from_input(Z)\n",
    "#   \n",
    "#   k = 0\n",
    "#   for i, x in enumerate(x_values):\n",
    "#       for j, y in enumerate(y_values):  \n",
    "#           x_recon = X_recon[k]\n",
    "#           k+=1\n",
    "#       \n",
    "#           x_recon=x_recon.reshape(28,28)\n",
    "#           image[(n - i - 1) * 28:(n - i) * 28, j * 28:(j + 1) * 28] = x_recon\n",
    "#   plt.imshow(image,cmap='gray')\n",
    "#   fig = plt.gcf()\n",
    "#   fig.set_size_inches(10,10)\n",
    "#   #fig.show()\n",
    "#   #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
