{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Coding a Variational AutoEncoder in Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The Variational AutoEncoder network is a network architecture that aims to compress a high dimensional input vector to a vector in a lower dimensional space (encoding). In order to do so the network compresses and decompresses the input during the learning process.\n",
    "\n",
    "![Center](images/vae_arch.png)\n",
    "\n",
    " We call the input of the network $X$ and the output $X'$. The train the network to obtain the optimal compression, i.e. when the difference between the input $X$ and the decompressed output $X'$ is minimum. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Let's first import some useful libraries, and define the Leaky_ReLU function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def lrelu(X, alpha=0.1):\n",
    "    return tf.maximum(alpha*X, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "Let's also import the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, _), (X_test, _) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and draw a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Sample: \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAABOhJREFUeJzt3bGOTVscwOG7r6uhkxAdlYyEIFFISDQq0WhVHkDiPbQeQSWZRCFCS6mhFJVCRaEiGrYXmLMmd86ZM+P8vq88/6zZq/llJbNyzp7mef4H2Hz/HvQGgPUQO0SIHSLEDhFih4j/1vmwaZr86x/22TzP006fO9khQuwQIXaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4Ra31lMz3nzp1bOPvw4cNw7cOHD4fzx48f72lPVU52iBA7RIgdIsQOEWKHCLFDhNghwj07++rKlSsLZ79//x6u/fz586q3k+ZkhwixQ4TYIULsECF2iBA7RIgdItyzs68uX768cPb9+/fh2mfPnq16O2lOdogQO0SIHSLEDhFihwixQ4SrN5Zy4cKF4fzBgwcLZ0+ePFn1dhhwskOE2CFC7BAhdogQO0SIHSLEDhHu2VnK1tbWcH78+PGFs6dPn656Oww42SFC7BAhdogQO0SIHSLEDhFih4hpnuf1PWya1vcw1uLt27fD+cmTJxfOdvsu/G4/Nc3O5nmedvrcyQ4RYocIsUOE2CFC7BAhdogQO0T4PjtDZ8+eHc6vXr06nH/8+HHhzD36ejnZIULsECF2iBA7RIgdIsQOEWKHCPfsDN28eXOp9V+/fl3RTliWkx0ixA4RYocIsUOE2CFC7BDh6o2hixcvLrX+0aNHK9oJy3KyQ4TYIULsECF2iBA7RIgdIsQOEV7ZHHft2rXh/MWLF8P5p0+fhvPr168vnP38+XO4lr3xymaIEztEiB0ixA4RYocIsUOE2CHC99njbt26NZyfOHFiOH/16tVw7i798HCyQ4TYIULsECF2iBA7RIgdIsQOEe7Z4y5dujSc7/Z7B9vb26vcDvvIyQ4RYocIsUOE2CFC7BAhdogQO0T43fgNd/r06eH8/fv3w/m3b9+G8/Pnz//vPbG//G48xIkdIsQOEWKHCLFDhNghwldcN9z9+/eH81OnTg3nL1++XOFuOEhOdogQO0SIHSLEDhFihwixQ4TYIcI9+4Y7c+bMUut3+4orfw8nO0SIHSLEDhFihwixQ4TYIULsEOGefcPduXNnqfXPnz9f0U44aE52iBA7RIgdIsQOEWKHCLFDhNghwj37Brhx48bC2W6vbKbDyQ4RYocIsUOE2CFC7BAhdohw9bYB7t69u3B25MiR4dp3794N52/evNnTnjh8nOwQIXaIEDtEiB0ixA4RYocIsUOEe/a/wLFjx4bz27dv7/lvb29vD+e/fv3a89/mcHGyQ4TYIULsECF2iBA7RIgdIsQOEdM8z+t72DSt72Eb5OjRo8P569evF86+fPkyXHvv3r3h/MePH8M5h888z9NOnzvZIULsECF2iBA7RIgdIsQOEWKHCPfssGHcs0Oc2CFC7BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQO0SIHSLW+spm4OA42SFC7BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4RYoeIP8ieku2KR0d8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = 2 #chose one sample in the training set\n",
    "\n",
    "X_train = X_train.astype(np.float32).reshape(-1,28*28)/X_train.max()\n",
    "X_test = X_test.astype(np.float32).reshape(-1,28*28)/X_train.max()\n",
    "\n",
    "print('\\n Sample: \\n')\n",
    "plt.imshow(X_test[sample].reshape(28,28), cmap='gray');\n",
    "plt.axis('off');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Layer class definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class provides the definition of a dense layer. A dense layer is characterized by its trainable weights:\n",
    "\n",
    "- a matrix W of dimensions (m_input, m_output)\n",
    "- a bias vector b of dimensions (m_output)\n",
    "\n",
    "The Layer class possesses the ```forward``` method which performs the linear forward propagation:\n",
    "\n",
    "$$\n",
    "Z_{out}= X_{in} \\times W(m_{in},m_{out})+b(m_{out})\n",
    "$$\n",
    "\n",
    "Code along with us and follow the provided instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \n",
    "    def __init__(self, mi, mo, name):\n",
    "        \n",
    "        \"\"\"\n",
    "        This is the init method of the class Layer:\n",
    "        It defines the class itself and contains the\n",
    "        definition of the parameters that will be used by\n",
    "        the other class' methods (i.e. forward)\n",
    "        \n",
    "        \"\"\"\n",
    "        # we are creating a dense layer, it's trainable parameters\n",
    "        # are the matrix W of dimensions (mi, mo) and the biases\n",
    "        # vectors that has dimension (mo)\n",
    "        \n",
    "        self.W = tf.get_variable(name='W_'+name,        # this name identifies uniquely the matrix W\n",
    "                                 shape=(mi, mo),           # this assigns to the matrix the needed shape\n",
    "                                 initializer=tf.glorot_uniform_initializer()) # you can choose among\n",
    "                                                                              # many different distributions\n",
    "                                                                              # to initialize your weights with\n",
    "        \n",
    "        self.b = tf.get_variable(name='b_'+name,        # this name identifies uniquely the bias vector b \n",
    "                                 shape=(mo,),              \n",
    "                                 initializer=tf.zeros_initializer(),) # biases can be initialized as zeros\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        \"\"\"\n",
    "        This is the forward propagation method of the class Layer\n",
    "        \"\"\"\n",
    "        # the method forward performs the following forward propagation \n",
    "        # operation on the input tensor X. Don't worry about back  \n",
    "        # propagation, tensorflow will handle it\n",
    "        \n",
    "        Z=tf.matmul(X,self.W)+self.b\n",
    "        \n",
    "        return Z\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Creating the AutoEncoder Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build the AutoEncoder as a class possessing several methods:\n",
    "\n",
    "```\n",
    "class AutoEncoder():\n",
    "\n",
    "    def __init__():\n",
    "    \n",
    "    def build_network():\n",
    "    \n",
    "    def decode():\n",
    "    \n",
    "    def encode():\n",
    "    \n",
    "    def fit():\n",
    "    \n",
    "```\n",
    "\n",
    "Code along with us and follow the provided instructions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(object):\n",
    "    \n",
    "    def __init__(self, input_dim, layer_dims, \n",
    "                learning_rate, epochs, batch_size):\n",
    "        \n",
    "        # Note: all of what has self in front of it will be reused outside the init method!\n",
    "        #       thus we make promote some constants to attributes of the class\n",
    "                \n",
    "        \"\"\"\n",
    "        The AutoEncoder (AE) class is initialized by providing the number of features of the \n",
    "        input (in this case 28*28) and a list of integers named layers_dims.\n",
    "        It contains the number of the nodes for the intermediate layers and is of the kind:\n",
    "        \n",
    "        layer_dims= [\n",
    "                     output_nodes_layer_1,\n",
    "                     output_nodes_layer_2,\n",
    "                     output_nodes_layer_3,\n",
    "                     ...\n",
    "                     output_nodes_layer_n\n",
    "                     ]\n",
    "                     \n",
    "        \"\"\"\n",
    "        # Write code here\n",
    "        self.layer_dims = layer_dims \n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.latent_dims = layer_dims[-1]\n",
    "        \"\"\"\n",
    "        \n",
    "        The AE architecture has a butterfly shape and thus we reverse the previous \n",
    "        list of shapes to obtain:\n",
    "        \n",
    "        reversed_layer_dims = [\n",
    "                               output_nodes_layer_(n-1),\n",
    "                               output_nodes_layer_(n-2),\n",
    "                               ...\n",
    "                               output_nodes_layer_2,\n",
    "                               output_nodes_layer_1,\n",
    "                               ]\n",
    "        \"\"\"\n",
    "        # Write code here\n",
    "        self.reversed_layer_dims= list(reversed(layer_dims)) #Note: We exclude layer n\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        Now we define the placeholder for our input tensor X_in, and create 2 empty lists\n",
    "        named encoder_layers and decoder_layers that will contain the layers of our AE\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #Write code here\n",
    "        self.X_in = tf.placeholder(tf.float32, shape=(None, input_dim))\n",
    "        \n",
    "        self.encoder_layers = []\n",
    "        self.decoder_layers = []\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        We miss one last ingredient to build our AE: a method that builds the network ,\n",
    "        it will be called build_network\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        # Write code here\n",
    "        self.build_network(input_dim)\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    Let's define what this build network method does:\n",
    "    \n",
    "    \"\"\"\n",
    "    #Write code here\n",
    "    def build_network(self, input_dim):\n",
    "        \n",
    "        \"\"\"\n",
    "        STEP 1: BUILDING THE NETWORK\n",
    "        \n",
    "        the method build_network creates the graph that represents the network. It \n",
    "        does this by making use of two for loops, the first defines the encoder layers\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        ENCODER LAYERS\n",
    "        \"\"\"\n",
    "        # Write code here\n",
    "        mi = input_dim    #input size\n",
    "        count=0           #set a counter for layers' names\n",
    "        \n",
    "        for mo in self.layer_dims[:-1]:           # loop over a list [mo_1, mo_2, ..., mo_(n-1), mo_n]\n",
    "                                             # of intermediate layer dimensions\n",
    "            \n",
    "            layer = Layer(mi, mo, str(count))     # for each intermediate layer dimension create a layer\n",
    "                                             # named 'count' with associated weights of shape W(mi, mo) b(mo)\n",
    "                \n",
    "            self.encoder_layers.append(layer)# add the created layer to the list of encoder layers\n",
    "            count+=1                         # and increase the counter\n",
    "            mi = mo                          # the input dimension mi of the next layer will be the ouput\n",
    "                                             # dimension of the previous one\n",
    "\n",
    "        self.encoder_layer_mu = Layer(mi, self.latent_dims, 'encoder_mu')\n",
    "        self.encoder_layer_si = Layer(mi, self.latent_dims, 'encoder_si')\n",
    "        \n",
    "        #Note: in this way the class is created independently of the size of the network! the actual\n",
    "        #      list of layer dimensions is given later!\n",
    "            \n",
    "        \"\"\"\n",
    "        DECODER LAYERS\n",
    "        \"\"\"\n",
    "        # We repeat the same operations, but in reverse order to create the right hand-side of the butterfly\n",
    "        # the decoder\n",
    "        \n",
    "        #Write code here\n",
    "        mi = self.latent_dims\n",
    "        for mo in self.reversed_layer_dims:   # loop over a list [mo_(n-1), mo_(n-2), ..., mo_2, mo_1]  \n",
    "            \n",
    "            layer = Layer(mi, mo, str(count))      # create the layer here\n",
    "            \n",
    "            self.decoder_layers.append(layer) # append the layer to the decoder layers list\n",
    "            count+=1                          # increase the counter and\n",
    "            mi = mo                           # update the output dimensions\n",
    "            \n",
    "        # Note that the last mo is not the input size, so, to compare\n",
    "        # the input of the network with the ouput \n",
    "        \n",
    "        output_layer = Layer(mi,input_dim, str(count))  # we'll have to create one last layer\n",
    "                                                    # of dimensions (mi, input_dim):\n",
    "        self.decoder_layers.append(output_layer)    # and append it to the list of decoder layers\n",
    "        \n",
    "    \n",
    "        \"\"\"\n",
    "        \n",
    "        STEP 2: BUILDING THE GRAPH\n",
    "        \n",
    "        Now that the network has been built, we want to \n",
    "        define how the input will propagate through the encoder and the decoder.\n",
    "        \n",
    "        The overall forward propagation of each side of the network is\n",
    "        the propagation through each single layer and is carried out by the\n",
    "        \"encode\" and \"decode\" functions:\n",
    "        \n",
    "        \"\"\"\n",
    "    \n",
    "        self.Z, encoded_mu, encoded_log_si = self.encode(self.X_in) # Propagation of the input through the encoder. \n",
    "                                        # This outputs the latent vector self.Z\n",
    "            \n",
    "        self.X_hat = self.decode(self.Z)    # Propagation of the latent vector self.Z\n",
    "                                        # through the decoder, last output is unactivated \n",
    "                                        # on purpose (will be clearer later)\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        STEP 3: DEFINING THE LOSS and THE OPTIMIZATION ALGORITHM\n",
    "        \n",
    "        With the two last steps we created the network and the graph of operations\n",
    "        that propagate the input until the network output X_hat. Now we have to define\n",
    "        The loss function, i.e. that positive number, function of the network\n",
    "        parameters that has to be minimized during training.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        LOSS\n",
    "        \"\"\"\n",
    "        \n",
    "        # write code here\n",
    "        encoder_kl = -0.5 * tf.reduce_mean(1 + 2*tf.log(encoded_log_si) - tf.square(encoded_mu) - tf.square(encoded_log_si))\n",
    "        consistency_loss = tf.reduce_sum(tf.square(self.X_in - self.X_hat))\n",
    "        \n",
    "        self.loss = encoder_kl + consistency_loss\n",
    "        # reduce_sum converts a tensor to a scalar sum of its components\n",
    "                    \n",
    "        \"\"\"\n",
    "        \n",
    "        OPTIMIZATION ALGORITHM\n",
    "        \n",
    "        Define the loss minimization algorithm, we chose Adaptive momentum optimizer (Adam) for \n",
    "        this purpose. Everytime train_op will be called, one step of gradient minimization will\n",
    "        be taken.\n",
    "        \"\"\"\n",
    "        # write code here\n",
    "        self.train_op = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "                                                #set the learning rate                #what to minimize\n",
    "        \n",
    "    \"\"\"\n",
    "    We now define the last two functions: encode and decode which we used previously\n",
    "    in STEP 2 \n",
    "    \"\"\"\n",
    "    # write code here\n",
    "    def encode(self, X):                                # This method takes as input a sample X\n",
    "                                                        # and outputs its encoded form Z\n",
    "        \n",
    "        output=X\n",
    "        \n",
    "        for layer in self.encoder_layers:          # This loop propagates the input X\n",
    "                                                        # through the encoder layers using\n",
    "            output = layer.forward(output)              # their forward method\n",
    "            output = lrelu(output)\n",
    "\n",
    "        mu = self.encoder_layer_mu.forward(output)\n",
    "        log_sigma = tf.nn.softplus(self.encoder_layer_si.forward(output))\n",
    "        eps = tf.random_normal(shape=tf.shape(self.latent_dims),\n",
    "                              mean=0.,stddev=1.)\n",
    "        \n",
    "        output = mu + tf.multiply(tf.exp(log_sigma), eps)\n",
    "        \n",
    "        return output, mu, log_sigma                    # vector in latent space, Z\n",
    "        \n",
    "    def decode(self,Z):                                 # This method takes a vector Z in the \n",
    "                                                        # latent space and decodes it to X_hat\n",
    "                                                        # living in the space of the samples\n",
    "        output=Z\n",
    "        \n",
    "        for layer in self.decoder_layers:               # Loop through the decoder network,  \n",
    "\n",
    "            output = layer.forward(output)\n",
    "            output = tf.nn.sigmoid(output)\n",
    "            \n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def fit(self, X, show_fig=True):                    # Fit method of the class AutoEncoder\n",
    "         \n",
    "        losses = []                                     # a list containing the losses\n",
    "        n_batches = len(X) // self.batch_size\n",
    "        \n",
    "        print(\"Training AutoEncoder\")\n",
    "        print(\"n_batches:\", n_batches)\n",
    "        for i in range(self.epochs):\n",
    "            \n",
    "            print(\"epoch:\", i)\n",
    "            np.random.shuffle(X)\n",
    "        \n",
    "            for j in range(n_batches):                  # the training cycle in one epoch:\n",
    "            \n",
    "                X_batch = X[j*self.batch_size:(j+1)*self.batch_size] # the batch to pass for computation\n",
    "                _, l, = self.session.run((self.train_op, self.loss), \n",
    "                                         feed_dict={self.X_in: X_batch}) #running one step of gradient\n",
    "                l /= self.batch_size                                     #minimization\n",
    "\n",
    "                losses.append(l)\n",
    "                \n",
    "                # let's add some intermediate printouts to see how the training is going\n",
    "                if j % 80*self.batch_size == 0:        \n",
    "\n",
    "                    print(\"iter: %d, cost: %.3f\" % (j, l))\n",
    "            \n",
    "        if show_fig:\n",
    "            plt.plot(losses)\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    def predict(self, X):                              # function to test the network once training\n",
    "                                                       # is completed\n",
    "        \n",
    "        return self.session.run(self.X_hat, feed_dict={self.X_in: X})\n",
    "    \n",
    "    def generate_sample(self):\n",
    "        \n",
    "        z = np.random.normal(size=(1,self.latent_dims))\n",
    "        \n",
    "        one_sample=self.session.run(self.X_hat, feed_dict={self.Z:z})\n",
    "\n",
    "        return one_sample\n",
    "    \n",
    "    def generate_sample_from_input(self, z):\n",
    "        \n",
    "        \n",
    "        one_sample=self.session.run(self.X_hat, feed_dict={self.Z:z})\n",
    "\n",
    "        return one_sample\n",
    "   \n",
    "    def set_session(self, session):\n",
    "        \n",
    "        self.session = session\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Test the AutoEncoder\n",
    "\n",
    "Now that the AutoEncoder class is defined, we create our own AutoEncoder and test how it works. As usual follow our coding and use the provided instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 784 and 2 for 'Normal/log_prob/standardize/sub' (op: 'Sub') with input shapes: [?,784], [?,2].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/keras_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1658\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1659\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1660\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimensions must be equal, but are 784 and 2 for 'Normal/log_prob/standardize/sub' (op: 'Sub') with input shapes: [?,784], [?,2].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-c512b5260c3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                               \u001b[0;31m# layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                   \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m       \u001b[0;31m# Set some values for the learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                   epochs=15, batch_size=100)  # the epochs and the batch_size\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-8ca5271edfeb>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dim, layer_dims, learning_rate, epochs, batch_size)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \"\"\"\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# Write code here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-8ca5271edfeb>\u001b[0m in \u001b[0;36mbuild_network\u001b[0;34m(self, input_dim)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;31m#consistency_loss = tf.reduce_sum(tf.square(self.X_in - self.X_hat))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         expected_log_likelihood = tf.reduce_mean(\n\u001b[0;32m--> 172\u001b[0;31m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_hat_distribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m               \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         )\n",
      "\u001b[0;32m~/miniconda3/envs/keras_env/lib/python3.6/site-packages/tensorflow/python/ops/distributions/distribution.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value, name)\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m     \"\"\"\n\u001b[0;32m--> 795\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras_env/lib/python3.6/site-packages/tensorflow/python/ops/distributions/distribution.py\u001b[0m in \u001b[0;36m_call_log_prob\u001b[0;34m(self, value, name, **kwargs)\u001b[0m\n\u001b[1;32m    775\u001b[0m           value, name=\"value\", preferred_dtype=self.dtype)\n\u001b[1;32m    776\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moriginal_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras_env/lib/python3.6/site-packages/tensorflow/python/ops/distributions/normal.py\u001b[0m in \u001b[0;36m_log_prob\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_unnormalized_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_normalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_log_cdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras_env/lib/python3.6/site-packages/tensorflow/python/ops/distributions/normal.py\u001b[0m in \u001b[0;36m_log_unnormalized_prob\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_log_unnormalized_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_log_normalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras_env/lib/python3.6/site-packages/tensorflow/python/ops/distributions/normal.py\u001b[0m in \u001b[0;36m_z\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;34m\"\"\"Standardize input `x` to a unit normal.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"standardize\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_inv_z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras_env/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    810\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras_env/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   9534\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9535\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 9536\u001b[0;31m         \"Sub\", x=x, y=y, name=name)\n\u001b[0m\u001b[1;32m   9537\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9538\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras_env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    786\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    787\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    789\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras_env/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/miniconda3/envs/keras_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3298\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3299\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3300\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3301\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3302\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1821\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1822\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1823\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m     \u001b[0;31m# Initialize self._outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1660\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1661\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1662\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1664\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 784 and 2 for 'Normal/log_prob/standardize/sub' (op: 'Sub') with input shapes: [?,784], [?,2]."
     ]
    }
   ],
   "source": [
    "_, input_dimensions = X_train.shape     # Lets get the input_dimensions from the dataset\n",
    "tf.reset_default_graph()                # Resetting any previously defined graph\n",
    "\n",
    "\"\"\"\n",
    "Create the AutoEncoder object\n",
    "\n",
    "dae = AutoEncoder() \n",
    "\n",
    "with aruguments it\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#write code here                \n",
    "dae = AutoEncoder(input_dim=input_dimensions, # Create your very own network here!!\n",
    "                  \n",
    "                  layer_dims=[500,300,100,2],   # Play with the layers' list, it will \n",
    "                                              # automatically define the intermediate\n",
    "                                              # layers.\n",
    "                  learning_rate=0.001,       # Set some values for the learning rate\n",
    "                  epochs=15, batch_size=100)  # the epochs and the batch_size\n",
    "\n",
    "\n",
    "\n",
    "init_op=tf.global_variables_initializer() # Define the network's variables initializer\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.10)\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options )) as sess: #Create a tensorflow session\n",
    "    \n",
    "    sess.run(init_op)     # Initialize the network variables in this sess\n",
    "    dae.set_session(sess) # Set it for the AutoEncoder you defined previously\n",
    "    \n",
    "    dae.fit(X_train)      # Call the fit method that you defined previously\n",
    "    \n",
    "\n",
    "    # Testing the created network performances on the test set!\n",
    "    done = True\n",
    "    while not done:\n",
    "        i = np.random.choice(len(X_test))         # pick a random sample of the test set\n",
    "        x = X_test[i]                             \n",
    "        y = dae.predict([x]).reshape(28, 28)      # propagate it through the network\n",
    "\n",
    "        plt.subplot(1,2,1)                        # Plot the original and the reconstructed\n",
    "        plt.imshow(x.reshape(28, 28), cmap='gray')\n",
    "        plt.title(\"Original\")\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.imshow(y, cmap='gray')\n",
    "        plt.title(\"Reconstruction\")\n",
    "        plt.show()\n",
    "\n",
    "        ans = input(\"Generate another?\")          # Stop the while loop with 'n' or 'N'\n",
    "        if ans and ans[0] in ('n' or 'N'):\n",
    "              done = True\n",
    "                \n",
    "    done = False\n",
    "    while not done:\n",
    "                       \n",
    "        y = dae.generate_sample();     # propagate it through the network\n",
    "\n",
    "        plt.imshow(y.reshape(28, 28), cmap='gray');\n",
    "        plt.title(\"Reconstruction\")\n",
    "        plt.show()\n",
    "\n",
    "        ans = input(\"Generate another?\")          # Stop the while loop with 'n' or 'N'\n",
    "        if ans and ans[0] in ('n' or 'N'):\n",
    "              done = True\n",
    "                \n",
    "    n = 10 #number of images per side\n",
    "    x_values = np.linspace(-3,3,n)\n",
    "    y_values = np.linspace(-3,3,n)\n",
    "    \n",
    "    image = np.empty((28*n,28*n))\n",
    "    \n",
    "    Z=[]\n",
    "    for i, x in enumerate(x_values):\n",
    "        for j, y in enumerate(y_values):\n",
    "            z=[x, y]\n",
    "            Z.append(z)\n",
    "        \n",
    "    X_recon = dae.generate_sample_from_input(Z)\n",
    "    \n",
    "    k = 0\n",
    "    for i, x in enumerate(x_values):\n",
    "        for j, y in enumerate(y_values):  \n",
    "            x_recon = X_recon[k]\n",
    "            k+=1\n",
    "        \n",
    "            x_recon=x_recon.reshape(28,28)\n",
    "            image[(n - i - 1) * 28:(n - i) * 28, j * 28:(j + 1) * 28] = x_recon\n",
    "    plt.imshow(image,cmap='gray')\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(10,10)\n",
    "    #fig.show()\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
